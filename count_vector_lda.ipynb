{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperação dos discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    files = list(pathlib.Path(\"./discursos/\").iterdir())\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"The directory 'discursos' was not found. It's necessary for the correct function of this file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos = []\n",
    "for df in df_list:\n",
    "    partidos.extend(df[\"sigla\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [df[\"transcricao\"].to_list() for df in df_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Sabe-se que O começo do discurso é feito com:\n",
    "\n",
    "O SR(A).NOME SOBRENOME(SIGLA-UF. Sem revisão do orador) - Começo do discurso\n",
    "\n",
    "Sabe-se portanto retirar este começo, utilizando-se do fato de haver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [preprocess(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, nlp, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(sent)\n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in [\"-PRON-\"] else \"\" for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "# Ideia possível, o token possui um campo head, se a head da palavra for Presidente e a palavra for Jair e/ou Bolsonaro, deve-se manter a palavra no texto e não descartá-la como está acontecendo com outras palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [lemmatization(discursos_, nlp, allowed_postags=[\"NOUN\", \"ADJ\", \"ADV\", \"VERB\", \"PUNCT\"]) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remoção das stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"portuguese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(discursos: list) -> list:\n",
    "    novos_discursos = []\n",
    "    for discurso in discursos:\n",
    "        discurso_split = discurso.split()\n",
    "        novo_discurso = []\n",
    "        for palavra in discurso_split:\n",
    "            if palavra not in stop_words:\n",
    "                novo_discurso.append(palavra)\n",
    "        novo_discurso = \" \".join(novo_discurso)\n",
    "        novos_discursos.append(novo_discurso)\n",
    "    return novos_discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [remove_stop_words(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(discursos: list):\n",
    "    discursos_lower = [discurso.lower() for discurso in discursos]\n",
    "    discursos_tokenized = [nltk.word_tokenize(discurso) for discurso in discursos_lower]\n",
    "    return discursos_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [tokenizer(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remoção das pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(discursos: list) -> list:\n",
    "    novos_discursos =[]\n",
    "    for discurso in discursos:\n",
    "        novo_discurso = []\n",
    "        for token in discurso:\n",
    "            if token not in string.punctuation:\n",
    "                novo_discurso.append(token)\n",
    "        novos_discursos.append(novo_discurso)\n",
    "    return novos_discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [remove_punct(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unifica_discursos(discursos: list) -> list:\n",
    "    novos_discursos =[\" \".join(discurso) for discurso in discursos]\n",
    "    return novos_discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [unifica_discursos(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correção de alguns bugs gerados pela Lemmatização do Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correcoes_lemmatizacao(discursos: list) -> list:\n",
    "    novos_discursos = [discurso.replace(\"morer\", \"moro\") for discurso in discursos]\n",
    "    return novos_discursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "discursos = [correcoes_lemmatizacao(discursos_) for discursos_ in discursos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearntext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearntext.CountVectorizer(analyzer='word', stop_words=None, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vectorizer(vectorizer: sklearntext.CountVectorizer, discursos: list):\n",
    "    matrix_discursos = vectorizer.fit_transform(discursos)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return matrix_discursos, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talvez usar dict comprehesion\n",
    "data_vectorized = []\n",
    "feature_names = []\n",
    "for discursos_ in discursos:\n",
    "    data, feature_name = data_vectorizer(vectorizer, discursos_)\n",
    "    data_vectorized.append(data)\n",
    "    feature_names.append(feature_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_transform_fit(data_vectorized: np.ndarray) -> LatentDirichletAllocation:\n",
    "    lda_model = LatentDirichletAllocation(learning_method=\"online\", random_state=100, batch_size=128, evaluate_every = -1, n_jobs= -1, n_components=10)\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_topics(feature_names: np.ndarray, lda_model_components: np.ndarray, n_words = 20) -> list:\n",
    "    keywords = np.array(feature_names)\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model_components:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models =[lda_transform_fit(data_vectorized_) for data_vectorized_ in data_vectorized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_keyowrds_lst = [transform_topics(feature_name, lda_models[i].components_, 15) for i, feature_name in enumerate(feature_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_keywords_df = [pd.DataFrame(topics_keywords) for topics_keywords in topics_keyowrds_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"./topics/lda/\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(topics_keywords_df):\n",
    "    topics_path = pathlib.Path(f\"./topics/lda/topics_{partidos[i]}.csv\")\n",
    "    df.columns = [\"Word\" + str(i) for i in range(df.shape[1])]\n",
    "    df.index = [\"Topic \" + str(i) for i in range(df.shape[0])]\n",
    "    df.to_csv(topics_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
